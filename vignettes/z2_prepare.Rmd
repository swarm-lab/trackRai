---
title: "2 - Prepare a dataset for training a YOLO model"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{2 - Prepare a dataset for training a YOLO model}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

`trackRai` provides four apps to: 

1. generate a training dataset suitable for training a YOLO model (this tutorial);
2. [train a YOLO model using that dataset](https://swarm-lab.github.io/trackRai/articles/z3_train.html);
3. [use the trained model to track objects in a video](https://swarm-lab.github.io/trackRai/articles/z4_track.html);
4. [visualize the results of the tracking](https://swarm-lab.github.io/trackRai/articles/z5_visualize.html). 

In this tutorial, we will discuss how to use the first app to automatically 
generate a dataset that you can then use to train a YOLO model. The idea here is
to use traditional computer vision techniques to isolate single instances of the 
objects that you would like to track, as well as the background over which these 
objects are moving. We will then use them to create composite images with random
arrangements of an arbitrary large number of these isolated objects. Using that 
approach, we can generate automatically a large number of training images for 
YOLO, without requiring any manual labelling.  

**Note:** this approach only works well if the background of the video is stable, 
with, ideally, no camera movement and no lighting variations. Good tracking 
results are not guaranteed otherwise. 

---

# 2.1 - Launch the preparation app

To launch the dataset preparation app, run the following in the R console:

```{r setup, eval = FALSE}
library(trackRai)
prepare()
```

This will open the app either in the viewer panel of RStudio and Positron, or in
your default internet browser. You can control where the app is opened using the
`launch.browser` parameter (see the documentation of shiny::runApp() for more
information).

---

# 2.2 - Tab 1: video module

Once the app opens, you will be presented with the "Video" tab. Click on the 
`Select video` button and navigate to the video that you would like to use for 
preparing the YOLO dataset. Once the video is selected, it will appear in the 
left panel of the app. 

![](../man/figures/prepare/1_video.jpg)

Below the video is a slider that you can use to control the video stream. The 
slider has three handles: 

+ the green handle allows you to navigate through the video to display a frame 
of your choice; 
+ the two grey handles allow you to restrict the processing of the video to a 
range of frames in the video. This can be convenient when only a portion of the 
video is usable, for instance. 

Once the video is loaded, the second tab of the app will become available and 
you can click on it to navigate there. 

---

# 2.3 - Tab 2: background module

![](../man/figures/prepare/2_background.jpg)

---

# 2.4 - Mask module

![](../man/figures/prepare/3_mask.jpg)

![](../man/figures/prepare/4_mask.jpg)

---

# 2.5 - Segmentation module

![](../man/figures/prepare/5_segmentation.jpg)

---

# 2.6 - Identification module

![](../man/figures/prepare/6_identification.jpg)

![](../man/figures/prepare/7_identification.jpg)

---

# 2.3 - Composite module

![](../man/figures/prepare/8_composite.jpg)

---