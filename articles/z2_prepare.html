<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<title>2 - Prepare a dataset for training a YOLO model • trackRai</title>
<!-- favicons --><link rel="icon" type="image/png" sizes="96x96" href="../favicon-96x96.png">
<link rel="icon" type="”image/svg+xml”" href="../favicon.svg">
<link rel="apple-touch-icon" sizes="180x180" href="../apple-touch-icon.png">
<link rel="icon" sizes="any" href="../favicon.ico">
<link rel="manifest" href="../site.webmanifest">
<script src="../deps/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="../deps/bootstrap-5.3.1/bootstrap.min.css" rel="stylesheet">
<script src="../deps/bootstrap-5.3.1/bootstrap.bundle.min.js"></script><link href="../deps/font-awesome-6.5.2/css/all.min.css" rel="stylesheet">
<link href="../deps/font-awesome-6.5.2/css/v4-shims.min.css" rel="stylesheet">
<script src="../deps/headroom-0.11.0/headroom.min.js"></script><script src="../deps/headroom-0.11.0/jQuery.headroom.min.js"></script><script src="../deps/bootstrap-toc-1.0.1/bootstrap-toc.min.js"></script><script src="../deps/clipboard.js-2.0.11/clipboard.min.js"></script><script src="../deps/search-1.0.0/autocomplete.jquery.min.js"></script><script src="../deps/search-1.0.0/fuse.min.js"></script><script src="../deps/search-1.0.0/mark.min.js"></script><!-- pkgdown --><script src="../pkgdown.js"></script><meta property="og:title" content="2 - Prepare a dataset for training a YOLO model">
</head>
<body>
    <a href="#main" class="visually-hidden-focusable">Skip to contents</a>


    <nav class="navbar navbar-expand-lg fixed-top bg-light" data-bs-theme="light" aria-label="Site navigation"><div class="container">

    <a class="navbar-brand me-2" href="../index.html">trackRai</a>

    <small class="nav-text text-muted me-auto" data-bs-toggle="tooltip" data-bs-placement="bottom" title="">0.0.0.9000</small>


    <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbar" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation">
      <span class="navbar-toggler-icon"></span>
    </button>

    <div id="navbar" class="collapse navbar-collapse ms-3">
      <ul class="navbar-nav me-auto">
<li class="nav-item"><a class="nav-link" href="../reference/index.html">Reference</a></li>
<li class="active nav-item dropdown">
  <button class="nav-link dropdown-toggle" type="button" id="dropdown-articles" data-bs-toggle="dropdown" aria-expanded="false" aria-haspopup="true">Articles</button>
  <ul class="dropdown-menu" aria-labelledby="dropdown-articles">
<li><a class="dropdown-item" href="../articles/z1_installyolo.html">1 - Install YOLO11</a></li>
    <li><a class="dropdown-item" href="../articles/z2_prepare.html">2 - Prepare a dataset for training a YOLO model</a></li>
    <li><a class="dropdown-item" href="../articles/z3_train.html">3 - Train a YOLO model</a></li>
    <li><a class="dropdown-item" href="../articles/z4_track.html">4 - Track objects in a video using a trained YOLO model</a></li>
    <li><a class="dropdown-item" href="../articles/z5_visualize.html">5 - Visualize the results of tracking</a></li>
  </ul>
</li>
      </ul>
<ul class="navbar-nav">
<li class="nav-item"><form class="form-inline" role="search">
 <input class="form-control" type="search" name="search-input" id="search-input" autocomplete="off" aria-label="Search site" placeholder="Search for" data-search-index="../search.json">
</form></li>
<li class="nav-item"><a class="external-link nav-link" href="https://github.com/swarm-lab/trackRai/" aria-label="GitHub"><span class="fa fab fa-github fa-lg"></span></a></li>
      </ul>
</div>


  </div>
</nav><div class="container template-article">




<div class="row">
  <main id="main" class="col-md-9"><div class="page-header">
      <img src="../logo.svg" class="logo" alt=""><h1>2 - Prepare a dataset for training a YOLO model</h1>
            
      
      <small class="dont-index">Source: <a href="https://github.com/swarm-lab/trackRai/blob/master/vignettes/z2_prepare.Rmd" class="external-link"><code>vignettes/z2_prepare.Rmd</code></a></small>
      <div class="d-none name"><code>z2_prepare.Rmd</code></div>
    </div>

    
    
<p><code>trackRai</code> provides four apps to:</p>
<ol style="list-style-type: decimal">
<li>generate a training dataset suitable for training a YOLO model (this
tutorial);</li>
<li>
<a href="https://swarm-lab.github.io/trackRai/articles/z3_train.html">train
a YOLO model using that dataset</a>;</li>
<li>
<a href="https://swarm-lab.github.io/trackRai/articles/z4_track.html">use
the trained model to track objects in a video</a>;</li>
<li>
<a href="https://swarm-lab.github.io/trackRai/articles/z5_visualize.html">visualize
the results of the tracking</a>.</li>
</ol>
<p>In this tutorial, we will discuss how to use the first app to
automatically generate a dataset that you can then use to train a YOLO
model. The idea here is to use traditional computer vision techniques to
isolate single instances of the objects that you would like to track, as
well as the background over which these objects are moving. We will then
use them to create composite images with random arrangements of an
arbitrary large number of these isolated objects. Using that approach,
we can generate automatically a large number of training images for
YOLO, without requiring any manual labelling.</p>
<p><strong>Note:</strong> this approach only works well if the
background of the video is stable, with, ideally, no camera movement and
no lighting variations. Good tracking results are not guaranteed
otherwise.</p>
<hr>
<div class="section level2">
<h2 id="launch-the-preparation-app">2.1 - Launch the preparation app<a class="anchor" aria-label="anchor" href="#launch-the-preparation-app"></a>
</h2>
<p>To launch the dataset preparation app, run the following in the R
console:</p>
<div class="sourceCode" id="cb1"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html" class="external-link">library</a></span><span class="op">(</span><span class="va"><a href="https://github.com/swarm-lab/trackRai" class="external-link">trackRai</a></span><span class="op">)</span></span>
<span><span class="fu"><a href="../reference/prepare.html">prepare</a></span><span class="op">(</span><span class="op">)</span></span></code></pre></div>
<p>This will open the app either in the viewer panel of RStudio and
Positron, or in your default internet browser. You can control where the
app is opened using the <code>launch.browser</code> parameter (see the
documentation of shiny::runApp() for more information).</p>
<hr>
</div>
<div class="section level2">
<h2 id="tab-1-video-module">2.2 - Tab 1: video module<a class="anchor" aria-label="anchor" href="#tab-1-video-module"></a>
</h2>
<p>Once the app opens, you will be presented with the “Video” tab. Click
on the <code>Select video</code> button and navigate to the video that
you would like to use for preparing the YOLO dataset. Once the video is
selected, it will appear in the left panel of the app.</p>
<p><img src="../reference/figures/prepare/1_video.jpg"></p>
<p>Below the video is a slider that you can use to control the video
stream. The slider has three handles:</p>
<ul>
<li>the green handle allows you to navigate through the video to display
a frame of your choice;</li>
<li>the two grey handles allow you to restrict the processing of the
video to a range of frames in the video. This can be convenient when
only a portion of the video is usable, for instance.</li>
</ul>
<p>Once the video is loaded, the second tab of the app will become
available and you can click on it to navigate there.</p>
<hr>
</div>
<div class="section level2">
<h2 id="tab-2-background-module">2.3 - Tab 2: background module<a class="anchor" aria-label="anchor" href="#tab-2-background-module"></a>
</h2>
<p>Once the video is loaded in memory, the first step of the process is
to load or reconstruct the background over which the objects are
moving.</p>
<p>If you already have an image of the background without the objects in
it, you can load it using the <code>Select existing background</code>
button. If not, you can attempt to reconstruct it by clicking on the
<code>Automatically estimate background</code> button.</p>
<p><img src="../reference/figures/prepare/2_background.jpg"></p>
<p>If you choose to reconstruct the background, you can select one of
the methods of reconstruction using the <code>Background type</code>
dropwdown menu:</p>
<ul>
<li>“mean” uses the average value of each pixel as background</li>
<li>“median” uses the median value of each pixel as background</li>
<li>“min” uses the minimum value of each pixel as background</li>
<li>“max” uses the maximum value of each pixel as background</li>
</ul>
<p>These statistics will be computed using several equally-spaced frames
of the video. You can choose the number of frames used for this process
with the <code>Number of frames for estimating background</code> slider.
More frames tend to yield a better result at the expense of a longer
computation time.</p>
<p>If objects are still present in the reconstructed background image
(for instance, because they did not move during the recording), you can
attempt to remove them by clicking on the
<code>Select ghost for removal</code> button. Once you have activated
the ghost removal mode, you can draw a polygon around the object to
remove (just click around the object to create the corners of the
polygon). Once you are done drawing the polygon, hit the
<code>return</code> key on your keyboard and the app will try to replace
the object with its best estimate of the local background.</p>
<p>Finally, you can save the background picture for later use by
clicking on the <code>Save background file</code> button.</p>
<p>Once a background image is created/loaded, the third tab of the app
will become available and you can click on it to navigate there.</p>
<p><strong>Note:</strong> the ghost removal mode is very basic and may
not yield good results with complex backgrounds. Another option is to
save the background file with the ghosts and use a more advanced image
editing software to remove them (for instance, Photoshop’s
<code>Remove tool</code> can give much better results).</p>
<hr>
</div>
<div class="section level2">
<h2 id="tab-3-mask-module">2.4 - Tab 3: mask module<a class="anchor" aria-label="anchor" href="#tab-3-mask-module"></a>
</h2>
<p>The second step of the process is to load or create an optional mask.
A mask can help restrict the area in which the app should look for
objects and, therefore, make the final results more accurate. By
default, the whole image is taken into account in the analyis.</p>
<p>If you already have a mask image, you can load it using the
<code>Select existing  mask</code> button. If not, you can create one as
follows.</p>
<p>First, click on the <code>Exclude all</code> button to exclude the
entirety of the image from the analysis. Then, click on either the
<code>Add polygon ROI</code> or <code>Add ellipse ROI</code> buttons,
making sure that the <code>Including</code> tick box is selected.</p>
<p>If you clicked the <code>Add polygon ROI</code> button, you will be
able to draw a polygon that encloses the region of the image you would
like to include in the analysis (just click in the image to create the
corners of the polygon). Once you are done drawing the polygon, hit the
<code>return</code> key on your keyboard to set the region of
interest.</p>
<p>If you clicked the <code>Add ellipse ROI</code> button, you will need
to select five points that will define the periphery of the ellipse.</p>
<p><img src="../reference/figures/prepare/3_mask.jpg"></p>
<p><img src="../reference/figures/prepare/4_mask.jpg"></p>
<p>You can also define multiple regions of interests by incrementing the
<code>ROI id</code> counter before creating a new region.</p>
<p>You can create more complex masks by excluding parts of the image
selectively if you tick the <code>Excluding</code> tick box.</p>
<p>Finally, you can save the mask picture for later use by clicking on
the <code>Save mask file</code> button.</p>
<p>Once a mask image is created/loaded, the fourth tab of the app will
become available and you can click on it to navigate there.</p>
<hr>
</div>
<div class="section level2">
<h2 id="tab-4-segmentation-module">2.5 - Tab 4: segmentation module<a class="anchor" aria-label="anchor" href="#tab-4-segmentation-module"></a>
</h2>
<p>The third step of the process is to determine a threshold that
differentiate the objects from the background. In this tab, what you are
seeing is the difference between a given frame of the video and the
background image loaded/created earlier.</p>
<p>By default, the app considers that the objects to detect appear
darker than the background in the video frames. If that is not the case,
you can tick the <code>Ligther</code> tick box if the objects appear
lighter than the background in the video frames, or the
<code>A bit of both</code> tick box if the objects have parts that are
darker than the background and others that are lighter.</p>
<p><img src="../reference/figures/prepare/5_segmentation.jpg"></p>
<p>You can then manually select a difference threshold by using the
slider at the bottom of the tab, or let the app decide for you by
clicking the <code>Autothreshold</code> button and selecting one of the
automatic threshold detection methods provided under the dropdown menu
on the right of the button.</p>
<p>Once a threshold has been set, the fifth tab of the app will become
available and you can click on it to navigate there.</p>
<hr>
</div>
<div class="section level2">
<h2 id="tab-5-identification-module">2.6 - Tab 5: identification module<a class="anchor" aria-label="anchor" href="#tab-5-identification-module"></a>
</h2>
<p>The fourth step of the process is to isolate single instances of the
objects in a subset of equally spaced frames in the video.</p>
<p>First, you will need to set a buffer around each of the detected
objects. The buffer should be large enough so that each object is
completely enclosed in its corresponding rectangle. Do not worry if
several objects are enclosed in the same rectangle; what matters is that
enough single instances are clearly separated from the others. You can
navigate through the subset of frames to check whether the selected
buffer size works well across all of them.</p>
<p><img src="../reference/figures/prepare/6_identification.jpg"></p>
<p>Once the buffer size is set, you will click on the
<code>Detect objects</code> button and the app will compute the widths
and heights of all the detected objects across the subset of frames. You
can choose the number of frames to be used yourself; more frames will
provide better statistics at the expense of longer computation time.
Once the statistics are computed, their distribution will be plotted in
the sidebar.</p>
<p><img src="../reference/figures/prepare/7_identification.jpg"></p>
<p>By default, the app will attempt to determine the best statistics
that corresponds to single instances of the objects. The single
instances detected that way will appear as green rectangles in the video
frame, the others will appear as red/orange rectangles. You can navigate
through the subset of frames to check whether the selected statistic
ranges work across the whole video. In particular, you should make sure
that all selected instances contain one, and only one object. If that is
not the case, you can untick the <code>Automatic object selection</code>
tick box and manually change the ranges to eliminate these instances.
You can also manually change the status of an instance (selected or
unselected) by directly clicking on it with your mouse cursor.</p>
<p>Once the objects have been selected, the last tab of the app will
become available and you can click on it to navigate there.</p>
<p><strong>Note:</strong> If you modify the buffer size or the number of
images used to detect the objects after computing the object statistics,
you will need to run the object detection again to take into account the
new parameters.</p>
<hr>
</div>
<div class="section level2">
<h2 id="tab-6-composite-module">2.7 - Tab 6: composite module<a class="anchor" aria-label="anchor" href="#tab-6-composite-module"></a>
</h2>
<p>The final step of the process is to generate the training dataset
itself. This will create composite images in which a set number of
objects will be printed over the background image, at random locations
and orientations within the boundary of the mask loaded/created earlier.
This will also create the bounding boxes of the objects that YOLO needs
to learn what the objects look like.</p>
<p>First, you will set the number of objects that will be included in
each of the composite images. The number of objects should be high
enough to result in a lot of random arrangements and overlaps so that
YOLO can generalize better to situations where objects touch or overlap
in the real video frames. You can also set a buffer zone to ensure that
the printed objects do not overlap too much with your mask
boundaries.</p>
<p>Next, you can add random noise to the composite images in order to
increase the generalizability of the YOLO training. You can also add
random gain and bias to change the contrast and luminosity of the the
composite images for ever more generalizability.</p>
<p>To check the effect of each parameter on the resulting composite
images, you can click on the <code>Generate test composite</code> button
to generate one or more sample composite images.</p>
<p>Once you are satisfied with the results, you can set the number of
images that will be used for training, validating, and testing the YOLO
model that you will train in a separate app. A recommend breakdown is to
reserve 70% of the images for training, and 15% for validating and
testing, respectively.</p>
<p>You can then generate the training dataset by clicking on the
<code>Generate YOLO dataset</code> button. This will bring up a file
manager and you can select the location where you would like the dataset
to be saved (a folder named <code>YOLO</code> will be created at that
location). Once the process terminates, you are done and you can close
the app. The next step will be <a href="https://swarm-lab.github.io/trackRai/articles/z3_train.html">training
a YOLO model</a>.</p>
<p><img src="../reference/figures/prepare/8_composite.jpg"></p>
<p><strong>Note:</strong> When generating the training dataset, you can
also ask the app to create a reframed video by ticking the tick box at
the bottom of the sidebar. This will prepare a video file in which
everything outside the mask will be removed and the dimensions of the
image will be set to optimize the video processing done by YOLO during
the tracking phase. This is completely optional and will add some
processing time, but performing the tracking on the reframed video can
be up to twice as fast as on the original video, depending on the number
of objects to track.</p>
<hr>
</div>
  </main><aside class="col-md-3"><nav id="toc" aria-label="Table of contents"><h2>On this page</h2>
    </nav></aside>
</div>



    <footer><div class="pkgdown-footer-left">
  <p>Developed by Simon Garnier, National Science Foundation, Award ID 2222418.</p>
</div>

<div class="pkgdown-footer-right">
  <p>Site built with <a href="https://pkgdown.r-lib.org/" class="external-link">pkgdown</a> 2.1.3.</p>
</div>

    </footer>
</div>





  </body>
</html>
